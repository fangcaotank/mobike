{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import Geohash as geo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import atan , pi ,sqrt, acos\n",
    "from xgboost import XGBClassifier \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_path = '../cache/'\n",
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/test.csv'\n",
    "split_time = '2017-05-23 00:00:00'\n",
    "flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#把test分成了4份\n",
    "def split_test():\n",
    "    test = pd.read_csv(test_path)\n",
    "    for i in range(4):\n",
    "        result_path = cache_path + 'test_%d.hdf'%(i)\n",
    "        if os.path.exists(result_path):\n",
    "            pass\n",
    "        else:\n",
    "            ts = test[test.index%4==i]\n",
    "            ts.to_hdf(result_path, 'w', complib='blosc', complevel=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取真实标签\n",
    "def get_label(data):\n",
    "    result_path = cache_path + 'label.pkl'\n",
    "    if os.path.exists(result_path):\n",
    "        label = pickle.load(open(result_path, 'rb+'))\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        val = train[(train['starttime']>= split_time)]\n",
    "        label = dict(zip(val['orderid'].values, val['geohashed_end_loc']))\n",
    "        pickle.dump(label, open(result_path, 'wb+'))\n",
    "    data['label'] = data['orderid'].map(label)\n",
    "    data['label'] = (data['label'] == data['geohashed_end_loc']).astype('int')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下部分，为特征构造部门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算方向夹角\n",
    "def get_azimuth(lon1, lat1, lon2, lat2):#lon1,测试点，#lon2基站信息\n",
    "    lat1 = float(lat1);lon1 = float(lon1);lat2 = float(lat2);lon2 = float(lon2)\n",
    "    dlon = lon1 - lon2\n",
    "    dlat = lat1 - lat2\n",
    "    hyp =sqrt(dlon**2+dlat**2)\n",
    "    if hyp == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        Z= dlat/hyp\n",
    "        rad =acos(Z)\n",
    "        deg = 180*rad/pi\n",
    "        if dlon >0:\n",
    "            return deg\n",
    "        elif dlon <0:\n",
    "            return 360-deg\n",
    "        elif dlon ==0 and dlat>0:\n",
    "            return 90\n",
    "        else:\n",
    "            return 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算两点之间距离\n",
    "def cal_distance(lat1,lon1,lat2,lon2):\n",
    "    lat1 = float(lat1);lon1 = float(lon1);lat2 = float(lat2);lon2 = float(lon2)\n",
    "    dx = np.abs(lon1 - lon2)  # 经度差\n",
    "    dy = np.abs(lat1 - lat2)  # 维度差\n",
    "    b = (lat1 + lat2) / 2.0\n",
    "    Lx = 6371004.0 * (dx / 57.2958) * np.cos(b / 57.2958)\n",
    "    Ly = 6371004.0 * (dy / 57.2958)\n",
    "    L = (Lx**2 + Ly**2) ** 0.5\n",
    "    return L\n",
    "\n",
    "# 计算两点之间的欧氏距离\n",
    "def get_distance(result):\n",
    "    locs = list(set(result['geohashed_start_loc']) | set(result['geohashed_end_loc']))\n",
    "    if np.nan in locs:\n",
    "        locs.remove(np.nan)\n",
    "    deloc = []\n",
    "    for loc in locs:\n",
    "        deloc.append((geo.decode_exactly(loc)[0],geo.decode_exactly(loc)[1]))\n",
    "    loc_dict = dict(zip(locs,deloc))\n",
    "    geohashed_loc = result[['geohashed_start_loc','geohashed_end_loc']].values\n",
    "    \n",
    "    azimuths = [];diss = [];slons = []; slats= [] ;elons = [];elats = []\n",
    "    \n",
    "    for i in geohashed_loc:\n",
    "        lat1, lon1 = loc_dict[i[0]]\n",
    "        lat2, lon2 = loc_dict[i[1]]\n",
    "        \n",
    "        slat,slon = geo.decode(i[0])\n",
    "        elat,elon = geo.decode(i[1])\n",
    "        \n",
    "        slat = float(slat);slon = float(slon);\n",
    "        elat = float(elat);elon = float(elon)\n",
    "        \n",
    "        dis = cal_distance(lat1, lon1, lat2, lon2)\n",
    "        azimuth = get_azimuth(lat1,lon1,lat2,lon2)\n",
    "              \n",
    "        azimuths.append(azimuth);diss.append(dis);\n",
    "        slons.append(slon);slats.append(slat);elons.append(elon);elats.append(elat)\n",
    "        \n",
    "    result['azimuth'] = azimuths;result['dis'] = diss;\n",
    "    result[\"slats\"] = slats; result[\"slons\"] = slons;result[\"elons\"] = elons;result[\"elats\"] = elats\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据方位角，每45度划分一个方向\n",
    "def azimuth_split(result):\n",
    "    cut_points = range(0,360,45)\n",
    "    cut_points = cut_points + [result[\"azimuth\"].max()]\n",
    "    labels = [\"0-45\",\"45-90\",\"90-135\",\"135-180\",\"180-225\",\"225-270\",\"270-315\",\"315-360\"]\n",
    "    df = pd.cut(result[\"azimuth\"], cut_points,labels=labels,include_lowest=True)\n",
    "    df = pd.get_dummies(df,prefix=\"azimuth_dis\")\n",
    "    df = pd.DataFrame(df.values * result[[\"dis\"]].values,columns=df.columns)\n",
    "    result = pd.concat([result,df],axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取用户历史行为次数\n",
    "def get_user_count(train,result):\n",
    "    user_count = train.groupby('userid',as_index=False)['geohashed_end_loc'].agg({'user_count':'count'})\n",
    "    result = pd.merge(result,user_count,on=['userid'],how='left')\n",
    "    return result\n",
    "\n",
    "# 获取用户去过某个地点历史行为次数\n",
    "def get_user_eloc_count(train, result):\n",
    "    user_eloc_count = train.groupby(['userid','geohashed_end_loc'],as_index=False)['userid'].agg({'user_eloc_count':'count'})\n",
    "    result = pd.merge(result,user_eloc_count,on=['userid','geohashed_end_loc'],how='left')\n",
    "    return result\n",
    "\n",
    "# 获取用户从某个地点出发的行为次数\n",
    "def get_user_sloc_count(train,result):\n",
    "    user_sloc_count = train.groupby(['userid','geohashed_start_loc'],as_index=False)['userid'].agg({'user_sloc_count':'count'})\n",
    "    user_sloc_count.rename(columns={'geohashed_start_loc':'geohashed_end_loc'},inplace=True)\n",
    "    result = pd.merge(result, user_sloc_count, on=['userid', 'geohashed_end_loc'], how='left')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取用户从这个路径走过几次\n",
    "def get_user_sloc_eloc_count(train,result):\n",
    "    user_count = train.groupby(['userid','geohashed_start_loc','geohashed_end_loc'],as_index=False)['userid'].agg({'user_sloc_eloc_count':'count'})\n",
    "    result = pd.merge(result,user_count,on=['userid','geohashed_start_loc','geohashed_end_loc'],how='left')\n",
    "    return result\n",
    "\n",
    "# 获取用户从这个路径折返过几次\n",
    "def get_user_eloc_sloc_count(train,result):\n",
    "    user_eloc_sloc_count = train.groupby(['userid','geohashed_start_loc','geohashed_end_loc'],as_index=False)['userid'].agg({'user_eloc_sloc_count':'count'})\n",
    "    user_eloc_sloc_count.rename(columns = {'geohashed_start_loc':'geohashed_end_loc','geohashed_end_loc':'geohashed_start_loc'},inplace=True)\n",
    "    result = pd.merge(result,user_eloc_sloc_count,on=['userid','geohashed_start_loc','geohashed_end_loc'],how='left')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取目标地点的热度(目的地)\n",
    "def get_eloc_count(train,result):\n",
    "    eloc_count = train.groupby('geohashed_end_loc', as_index=False)['userid'].agg({'eloc_count': 'count'})\n",
    "    result = pd.merge(result, eloc_count, on='geohashed_end_loc', how='left')\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 获取目标地点的热度(出发地地)\n",
    "def get_sloc_count(train,result):\n",
    "    sloc_count = train.groupby('geohashed_start_loc', as_index=False)['userid'].agg({'sloc_count': 'count'})\n",
    "    result = pd.merge(result, sloc_count, on='geohashed_start_loc', how='left')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得某个时间段，达到某个地方，或者是从某个时间段出发的信息\n",
    "def hour_loc_count(train,result):\n",
    "    result['hour'] = pd.to_datetime(result['starttime']).map(lambda x: x.strftime('%H'))\n",
    "    train['hour'] = pd.to_datetime(train['starttime']).map(lambda x: x.strftime('%H'))\n",
    "            \n",
    "    hour_sloc_count = train.groupby([\"hour\",\"geohashed_start_loc\"],as_index=False)[\"userid\"].agg({'hour_sloc_count':'count'})\n",
    "    #某时，从某个地方出发的次数\n",
    "    hour_eloc_count = train.groupby([\"hour\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'hour_eloc_count':'count'})\n",
    "    #某时，到达某地的次数\n",
    "    hour_sloc_eloc_count = train.groupby([\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'hour_sloc_eloc_count':'count'})\n",
    "    #某时，某地出发，到达某地的次数\n",
    "    user_hour = train.groupby([\"userid\",\"hour\"],as_index=False)[\"userid\"].agg({'hour_user_count':'count'})\n",
    "    #用户从某个时间出发的次数\n",
    "    user_hour_eloc = train.groupby([\"userid\",\"hour\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'user_hour_eloc':'count'})\n",
    "    #用户某时到某地的次数\n",
    "    user_hour_sloc = train.groupby([\"userid\",\"hour\",\"geohashed_start_loc\"],as_index=False)[\"userid\"].agg({'user_hour_sloc':'count'})\n",
    "    #用户某时从某地出发的次数\n",
    "    user_hour_sloc_eloc = train.groupby([\"userid\",\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'user_hour_sloc_eloc':'count'})\n",
    "    #用户某时从某地出发到达某地的次数\n",
    "    \n",
    "    result = pd.merge(result, hour_sloc_count, on=[\"hour\",\"geohashed_start_loc\"], how='left')\n",
    "    result = pd.merge(result, hour_eloc_count, on=[\"hour\",\"geohashed_end_loc\"], how='left')\n",
    "    result = pd.merge(result, hour_sloc_eloc_count, on=[\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"], how='left')\n",
    "    result = pd.merge(result, user_hour, on=[\"userid\",\"hour\"], how='left')\n",
    "    result = pd.merge(result, user_hour_eloc, on=[\"userid\",\"hour\",\"geohashed_end_loc\"], how='left')\n",
    "    result = pd.merge(result, user_hour_sloc, on=[\"userid\",\"hour\",\"geohashed_start_loc\"], how='left')\n",
    "    result = pd.merge(result, user_hour_sloc_eloc, on=[\"userid\",\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"], how='left')\n",
    "    \n",
    "    result[\"hour\"] = result[\"hour\"].astype(int)\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得距离和频次的比值特征\n",
    "def dis_by_count(result):\n",
    "    result[\"dis_by_count\"] = result[\"dis\"] / (result[\"user_sloc_eloc_count\"] + 1)**1.1\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下部分，为样本构造部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得训练集的basic\n",
    "def get_train_basic():\n",
    "    print \"获得训练集样本基础\"\n",
    "    bs_feat = ['orderid','geohashed_end_loc']\n",
    "    result_path = cache_path + 'get_train_basic_%s.hdf' %(\"train_bs\")\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        val = train[(train['starttime']>= split_time)]\n",
    "        #train = train[(train['starttime']< split_time)]\n",
    "                \n",
    "        user_end_loc = get_user_end_loc(train, val)# 根据用户历史目的地点添加样本\n",
    "        user_start_loc = get_user_start_loc(train, val)# 根据用户历史起始地点添加样本\n",
    "        loc_to_loc = get_loc_to_loc(train, val)# 筛选起始地点去向最多的3个地点\n",
    "        bike_end_loc = get_bike_end_loc(train, val)#车ID中，多次重复去同一个地方的样本\n",
    "        user_go_back = get_user_go_back(train, val)#用户存在往返的样本 \n",
    "        \n",
    "        result = pd.concat([user_end_loc[bs_feat],\n",
    "                            user_start_loc[bs_feat],\n",
    "                            loc_to_loc[bs_feat],\n",
    "                            bike_end_loc[bs_feat],\n",
    "                            user_go_back[bs_feat],]).drop_duplicates()\n",
    "        \n",
    "        val.drop([\"geohashed_end_loc\"],axis=1,inplace=True)\n",
    "        result = pd.merge(result,val,on=\"orderid\",how=\"left\")\n",
    "        result = result[result['geohashed_end_loc'] != result['geohashed_start_loc']]\n",
    "        result = result[(~result['geohashed_end_loc'].isnull()) & (~result['geohashed_start_loc'].isnull())]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "##获得训练集的basic\n",
    "def get_test_basic(test,i):\n",
    "    print \"获得测试集样本基础\"\n",
    "    bs_feat = ['orderid','geohashed_end_loc']\n",
    "    result_path = cache_path + 'get_test_basic_%d.hdf' %(i)\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        #train = train[(train['starttime']< split_time)]\n",
    "        \n",
    "        user_end_loc = get_user_end_loc(train, test) # 根据用户历史目的地点添加样本\n",
    "        user_start_loc = get_user_start_loc(train, test)# 根据用户历史起始地点添加样本 \n",
    "        loc_to_loc = get_loc_to_loc(train, test)  # 筛选起始地点去向最多的3个地点\n",
    "        bike_end_loc = get_bike_end_loc(train, test)#车ID中，多次重复去同一个地方的样本\n",
    "        user_go_back = get_user_go_back(train, test)#用户存在往返的样本 \n",
    "        # 汇总样本id\n",
    "        result = pd.concat([user_end_loc[bs_feat],\n",
    "                            user_start_loc[bs_feat],\n",
    "                            loc_to_loc[bs_feat],\n",
    "                            bike_end_loc[bs_feat],\n",
    "                            user_go_back[bs_feat],])\n",
    "        \n",
    "        result=result.drop_duplicates()\n",
    "                \n",
    "        result = pd.merge(result,test,on=\"orderid\",how=\"left\")\n",
    "        result = result[result['geohashed_end_loc'] != result['geohashed_start_loc']]\n",
    "        result = result[(~result['geohashed_end_loc'].isnull()) & (~result['geohashed_start_loc'].isnull())]\n",
    "        \n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 将用户骑行过目的的地点加入样本\n",
    "def get_user_end_loc(train,sample):\n",
    "    result_path = cache_path + 'user_end_loc_%d.hdf' %(train.shape[0]*sample.orderid.values[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        user_eloc = train[['userid','geohashed_end_loc']].drop_duplicates()\n",
    "        result = pd.merge(sample[['orderid','userid']],user_eloc,on='userid',how='left')\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 将用户骑行过出发的地点加入样本\n",
    "def get_user_start_loc(train,sample):\n",
    "    result_path = cache_path + 'user_start_loc_%d.hdf' %(train.shape[0]*sample.orderid.values[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        test = pd.read_csv(test_path)\n",
    "        user_sloc = pd.concat([train[['userid', 'geohashed_start_loc']],\n",
    "                               test[['userid', 'geohashed_start_loc']]],\n",
    "                              axis=0).drop_duplicates()\n",
    "        result = pd.merge(sample[['orderid', 'userid']], user_sloc, on='userid', how='left')\n",
    "        result.rename(columns={'geohashed_start_loc':'geohashed_end_loc'},inplace=True)\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 筛选起始地点去向最多的5个地点\n",
    "def get_loc_to_loc(train,sample):\n",
    "    result_path = cache_path + 'loc_to_loc_%d.hdf' %(train.shape[0]*sample.orderid.values[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        sloc_eloc_count = train.groupby(['geohashed_start_loc', 'geohashed_end_loc'],as_index=False)['geohashed_end_loc'].agg({'sloc_eloc_count':'count'})\n",
    "        sloc_eloc_count.sort_values('sloc_eloc_count',inplace=True)\n",
    "        sloc_eloc_count = sloc_eloc_count.groupby('geohashed_start_loc').tail(5)\n",
    "        \n",
    "        result = pd.merge(sample[['orderid', 'geohashed_start_loc']], sloc_eloc_count, on='geohashed_start_loc', how='left')\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 将车ID多次重复去一个地方的样本取出\n",
    "def get_bike_end_loc(train,sample):\n",
    "    result_path = cache_path + 'bike_end_loc_%d.hdf' %(train.shape[0]*sample.orderid.values[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        bike_eloc = train.groupby([\"bikeid\",\"geohashed_end_loc\"],as_index=False)[\"geohashed_end_loc\"].agg({\"bike_count\":\"count\"})\n",
    "        bike_eloc = bike_eloc[bike_eloc.bike_count>2][[\"bikeid\",\"geohashed_end_loc\"]]\n",
    "        result = pd.merge(sample[['orderid','bikeid']],bike_eloc,on='bikeid',how='left')\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "#将用户往返过的地方，放入样本\n",
    "def get_user_go_back(train,sample):\n",
    "    result_path = cache_path + 'user_go_back_%d.hdf' %(train.shape[0]*sample.orderid.values[0])\n",
    "    \n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        user_go_back = pd.merge(train[[\"orderid\",\"userid\",\"geohashed_start_loc\",\"geohashed_end_loc\"]],\n",
    "                                train[[\"orderid\",\"userid\",\"geohashed_start_loc\",\"geohashed_end_loc\"]],\n",
    "                                left_on=[\"userid\",\"geohashed_start_loc\",\"geohashed_end_loc\"],\n",
    "                                right_on=[\"userid\",\"geohashed_end_loc\",\"geohashed_start_loc\"],\n",
    "                                how =\"inner\")\n",
    "        \n",
    "        user_go_back = user_go_back[[\"userid\",\"geohashed_start_loc_x\",\"geohashed_end_loc_x\"]].drop_duplicates()\n",
    "        \n",
    "        result1 = pd.merge(sample[['orderid','userid',\"geohashed_start_loc\"]],\n",
    "                          user_go_back,\n",
    "                          left_on = [\"userid\",\"geohashed_start_loc\"],\n",
    "                          right_on= [\"userid\",\"geohashed_start_loc_x\"],\n",
    "                          how=\"inner\")\n",
    "        result1.rename(columns={'geohashed_end_loc_x':'geohashed_end_loc'},inplace=True)\n",
    "        \n",
    "        result2 = pd.merge(sample[['orderid','userid',\"geohashed_start_loc\"]],\n",
    "                          user_go_back,\n",
    "                          left_on = [\"userid\",\"geohashed_start_loc\"],\n",
    "                          right_on= [\"userid\",\"geohashed_end_loc_x\"],\n",
    "                          how=\"inner\")\n",
    "        result2.rename(columns={'geohashed_start_loc_x':'geohashed_end_loc'},inplace=True)\n",
    "        \n",
    "        result = pd.concat([result1,result2],axis=0)\n",
    "        \n",
    "        result = result[['orderid', 'geohashed_end_loc']].drop_duplicates()\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tr_data():\n",
    "    print \"获得训练集数据\"\n",
    "    result_path = cache_path + 'make_tr_data.hdf'\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "   \n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        train = train[(train['starttime']< split_time)]\n",
    "        \n",
    "        result = get_train_basic()\n",
    "        result = hour_loc_count(train,result)                                   # 处理时间相关特征\n",
    "        result = get_user_count(train,result)                                   # 获取用户历史行为次数\n",
    "        result = get_user_eloc_count(train, result)                             # 获取用户去过这个地点几次\n",
    "        result = get_user_sloc_count(train, result)                             # 获取用户从目的地点出发过几次\n",
    "        result = get_user_sloc_eloc_count(train, result)                        # 获取用户从这个路径走过几次\n",
    "        result = get_user_eloc_sloc_count(train, result)                        # 获取用户从这个路径折返过几次\n",
    "        result = get_eloc_count(train, result)                                  # 获取目的地点的热度(目的地)\n",
    "        result = get_sloc_count(train, result)                                 \n",
    "        result = get_distance(result)                                           # 获取起始点和最终地点的欧式距离                                           \n",
    "        result = result.fillna(0)\n",
    "        result = dis_by_count(result)\n",
    "        result = azimuth_split(result)\n",
    "        \n",
    "        result = get_label(result)\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ts_data(test,i):\n",
    "    print \"获得测试集数据\"\n",
    "    result_path = cache_path + 'make_ts_data_%d.hdf'%(i)\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        train = train[(train['starttime']< split_time)]\n",
    "        \n",
    "        result = get_test_basic(test,i)\n",
    "        result = hour_loc_count(train,result)                                   # 处理时间相关特征\n",
    "        result = get_user_count(train,result)                                   # 获取用户历史行为次数\n",
    "        result = get_user_eloc_count(train, result)                             # 获取用户去过这个地点几次\n",
    "        result = get_user_sloc_count(train, result)                             # 获取用户从目的地点出发过几次\n",
    "        result = get_user_sloc_eloc_count(train, result)                        # 获取用户从这个路径走过几次\n",
    "        result = get_user_eloc_sloc_count(train, result)                        # 获取用户从这个路径折返过几次\n",
    "        result = get_eloc_count(train, result)                                  # 获取目的地点的热度(目的地)\n",
    "        result = get_sloc_count(train, result)\n",
    "        result = get_distance(result)                                           # 获取起始点和最终地点的欧式距离\n",
    "        result = result.fillna(0)\n",
    "        result = dis_by_count(result)\n",
    "        result = azimuth_split(result)\n",
    "        \n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#训练样本 \n",
    "def train_model():\n",
    "    \n",
    "    train_feat = make_tr_data()\n",
    "    \n",
    "    feat = [\"orderid\",\"geohashed_end_loc\",\"geohashed_start_loc\",\"userid\",\"bikeid\",\"starttime\"];label = [\"label\"]\n",
    "    predictors = [_ for _ in train_feat.columns if _ not in feat+label]\n",
    "    \n",
    "    print \"开始训练\"\n",
    "    result_path = cache_path + 'clf.pkl'\n",
    "    if os.path.exists(result_path):\n",
    "        clf = pickle.load(open(result_path, 'rb+'))\n",
    "    else:\n",
    "        split_test()\n",
    "        clf = XGBClassifier(objective ='binary:logistic',\n",
    "                                        learning_rate = 0.1,\n",
    "                                        silent=0,\n",
    "                                        n_estimators= 300,\n",
    "                                        max_depth = 10,\n",
    "                                        min_child_weight = 2,\n",
    "                                        gamma = 30,\n",
    "                                        reg_alpha = 10,\n",
    "                                        reg_lambda = 50,                        \n",
    "                                        subsample = 0.886, \n",
    "                                        colsample_bytree = 0.886,\n",
    "                                        scale_pos_weight = 10,\n",
    "                                        nthread =4)\n",
    "\n",
    "        X_train = train_feat[predictors]\n",
    "        y_train = train_feat[label]\n",
    "\n",
    "        clf.fit(X_train, y_train,\n",
    "                eval_set=[(X_train,y_train)], \n",
    "                eval_metric= \"auc\",\n",
    "                early_stopping_rounds=50)\n",
    "        pickle.dump(clf, open(result_path, 'wb+'))\n",
    "    return clf,predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分组排序\n",
    "def rank(data, feat1, feat2):\n",
    "    data.sort_values([feat1,feat2],inplace=True,ascending=False)\n",
    "    data['rank'] = range(data.shape[0])\n",
    "    min_rank = data.groupby(feat1,as_index=False)['rank'].agg({'min_rank':'min'})\n",
    "    data = pd.merge(data,min_rank,on=feat1,how='left')\n",
    "    data['rank'] = data['rank'] - data['min_rank']\n",
    "    del data['min_rank']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对结果进行整理\n",
    "def reshape(pred):\n",
    "    print \"组合结果\"\n",
    "    result = pred[[\"orderid\",\"geohashed_end_loc\",\"pred\"]]\n",
    "    result = rank(result,'orderid','pred')\n",
    "    result = result[result['rank']<3][['orderid','geohashed_end_loc','rank']]\n",
    "    result = result.set_index(['orderid','rank']).unstack()\n",
    "    result.reset_index(inplace=True)\n",
    "    result['orderid'] = result['orderid'].astype('int')\n",
    "    result.columns = ['orderid', 0, 1, 2]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sub():\n",
    "    clf,predictors = train_model()\n",
    "    results = []\n",
    "    for i in range(4):\n",
    "        print \"进行第%d个测试集预测\"%(i)\n",
    "        result_path = cache_path + 'test_%d.hdf'%(i)\n",
    "        test = pd.read_hdf(result_path, 'w')\n",
    "        \n",
    "        test_feat = make_ts_data(test,i)\n",
    "                \n",
    "        test_feat['pred'] = clf.predict_proba(test_feat[predictors])[:,1]\n",
    "        result = reshape(test_feat)\n",
    "        results.append(result)\n",
    "        gc.collect()\n",
    "        \n",
    "    result = pd.concat(results,axis=0)\n",
    "    print \"测试集合并完成\"\n",
    "    test = pd.read_csv(test_path)[['orderid']]\n",
    "    result = pd.merge(test,result,on='orderid',how='left')\n",
    "    result.fillna('0',inplace=True)\n",
    "    result.to_csv('../sub/result.csv',index=False,header=False)\n",
    "    \n",
    "    import matplotlib.pylab as plt\n",
    "    plt.figure(figsize=(20,4))\n",
    "    feat_imp = pd.Series(clf.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获得训练集数据\n",
      "获得训练集样本基础\n",
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until validation_0 error hasn't decreased in 50 rounds.\n",
      "[0]\tvalidation_0-auc:0.877641\n",
      "[1]\tvalidation_0-auc:0.888368\n",
      "[2]\tvalidation_0-auc:0.895950\n",
      "[3]\tvalidation_0-auc:0.895819\n",
      "[4]\tvalidation_0-auc:0.898747\n",
      "[5]\tvalidation_0-auc:0.900748\n",
      "[6]\tvalidation_0-auc:0.901863\n",
      "[7]\tvalidation_0-auc:0.902370\n",
      "[8]\tvalidation_0-auc:0.903172\n",
      "[9]\tvalidation_0-auc:0.903491\n",
      "[10]\tvalidation_0-auc:0.903585\n",
      "[11]\tvalidation_0-auc:0.903713\n",
      "[12]\tvalidation_0-auc:0.903905\n",
      "[13]\tvalidation_0-auc:0.904008\n",
      "[14]\tvalidation_0-auc:0.904360\n",
      "[15]\tvalidation_0-auc:0.904383\n",
      "[16]\tvalidation_0-auc:0.905102\n",
      "[17]\tvalidation_0-auc:0.905231\n",
      "[18]\tvalidation_0-auc:0.905864\n",
      "[19]\tvalidation_0-auc:0.905982\n",
      "[20]\tvalidation_0-auc:0.906040\n",
      "[21]\tvalidation_0-auc:0.906151\n",
      "[22]\tvalidation_0-auc:0.906179\n",
      "[23]\tvalidation_0-auc:0.906850\n",
      "[24]\tvalidation_0-auc:0.906887\n",
      "[25]\tvalidation_0-auc:0.907100\n",
      "[26]\tvalidation_0-auc:0.907140\n",
      "[27]\tvalidation_0-auc:0.907443\n",
      "[28]\tvalidation_0-auc:0.907464\n",
      "[29]\tvalidation_0-auc:0.907500\n",
      "[30]\tvalidation_0-auc:0.908386\n",
      "[31]\tvalidation_0-auc:0.908564\n",
      "[32]\tvalidation_0-auc:0.909172\n",
      "[33]\tvalidation_0-auc:0.909330\n",
      "[34]\tvalidation_0-auc:0.909339\n",
      "[35]\tvalidation_0-auc:0.909435\n",
      "[36]\tvalidation_0-auc:0.909430\n",
      "[37]\tvalidation_0-auc:0.909565\n",
      "[38]\tvalidation_0-auc:0.909637\n",
      "[39]\tvalidation_0-auc:0.909781\n",
      "[40]\tvalidation_0-auc:0.910006\n",
      "[41]\tvalidation_0-auc:0.910296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-65ac0a28c527>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mget_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-1893246298e4>\u001b[0m in \u001b[0;36mget_sub\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"进行第%d个测试集预测\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-b5e5dec2e051>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"auc\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 early_stopping_rounds=50)\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\xgboost-0.4-py2.7.egg\\xgboost\\sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[0;32m    341\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                               verbose_eval=verbose)\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\xgboost-0.4-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mnboost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mbst_eval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\xgboost-0.4-py2.7.egg\\xgboost\\core.pyc\u001b[0m in \u001b[0;36meval_set\u001b[1;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[0;32m    753\u001b[0m             _check_call(_LIB.XGBoosterEvalOneIter(self.handle, iteration,\n\u001b[0;32m    754\u001b[0m                                                   \u001b[0mdmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                                                   ctypes.byref(msg)))\n\u001b[0m\u001b[0;32m    756\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    get_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "方总投诉反馈：\n",
    "1、玲珑街道徐家坞208号（三幢联排），山坳内阻挡导致弱覆盖（塔下黑），4G:室内-脱网,室外-95dBm,2G:室内脱网，室外：-85dBm。\n",
    "用户无安装皮，飞蜂窝解决信号问题的意向，要求村落整体覆盖。\n",
    "投诉点附近约50米山顶有移动4G基站，基站是覆盖高速道路的，天馈不太适宜调整。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
