{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import Geohash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_path = '../cache/'\n",
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/test.csv'\n",
    "split_time = '2017-05-23 00:00:00'\n",
    "flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取真实标签\n",
    "def get_label(data):\n",
    "    result_path = cache_path + 'true.pkl'\n",
    "    if os.path.exists(result_path):\n",
    "        true = pickle.load(open(result_path, 'rb+'))\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "           \n",
    "        true = dict(zip(train['orderid'].values, train['geohashed_end_loc']))\n",
    "        pickle.dump(true, open(result_path, 'wb+'))\n",
    "    data['label'] = data['orderid'].map(true)\n",
    "    data['label'] = (data['label'] == data['geohashed_end_loc']).astype('int')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下部分，为特征构造部门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算两点之间距离\n",
    "def cal_distance(lat1,lon1,lat2,lon2):\n",
    "    lat1 = float(lat1);lon1 = float(lon1);lat2 = float(lat2);lon2 = float(lon2)\n",
    "    dx = np.abs(lon1 - lon2)  # 经度差\n",
    "    dy = np.abs(lat1 - lat2)  # 维度差\n",
    "    b = (lat1 + lat2) / 2.0\n",
    "    Lx = 6371004.0 * (dx / 57.2958) * np.cos(b / 57.2958)\n",
    "    Ly = 6371004.0 * (dy / 57.2958)\n",
    "    L = (Lx**2 + Ly**2) ** 0.5\n",
    "    return L\n",
    "\n",
    "# 计算两点之间的欧氏距离\n",
    "def get_distance(result):\n",
    "    locs = list(set(result['geohashed_start_loc']) | set(result['geohashed_end_loc']))\n",
    "    if np.nan in locs:\n",
    "        locs.remove(np.nan)\n",
    "    deloc = []\n",
    "    for loc in locs:\n",
    "        deloc.append(Geohash.decode(loc))\n",
    "    loc_dict = dict(zip(locs,deloc))\n",
    "    geohashed_loc = result[['geohashed_start_loc','geohashed_end_loc']].values\n",
    "    \n",
    "    distance = []\n",
    "    Manhattan = []\n",
    "    for i in geohashed_loc:\n",
    "        lat1, lon1 = loc_dict[i[0]]\n",
    "        lat2, lon2 = loc_dict[i[1]]\n",
    "        \n",
    "        dis1 = cal_distance(lat1, lon1, lat2, lon1)\n",
    "        dis2 = cal_distance(lat2, lon1, lat2, lon2)\n",
    "        \n",
    "        Manhattan_dis = dis1 + dis2\n",
    "        \n",
    "        #line_dis = cal_distance(lat1,lon1,lat2,lon2)\n",
    "        #distance.append(line_dis)\n",
    "        Manhattan.append(Manhattan_dis)\n",
    "        \n",
    "    #result['distance'] = distance\n",
    "    result['Manhattan'] = Manhattan\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取用户历史行为次数\n",
    "def get_user_count(train,result):\n",
    "    user_count = train.groupby('userid',as_index=False)['geohashed_end_loc'].agg({'user_count':'count'})\n",
    "    result = pd.merge(result,user_count,on=['userid'],how='left')\n",
    "    return result\n",
    "\n",
    "# 获取用户去过某个地点历史行为次数\n",
    "def get_user_eloc_count(train, result):\n",
    "    user_eloc_count = train.groupby(['userid','geohashed_end_loc'],as_index=False)['userid'].agg({'user_eloc_count':'count'})\n",
    "    result = pd.merge(result,user_eloc_count,on=['userid','geohashed_end_loc'],how='left')\n",
    "    return result\n",
    "\n",
    "# 获取用户从某个地点出发的行为次数\n",
    "def get_user_sloc_count(train,result):\n",
    "    user_sloc_count = train.groupby(['userid','geohashed_start_loc'],as_index=False)['userid'].agg({'user_sloc_count':'count'})\n",
    "    user_sloc_count.rename(columns={'geohashed_start_loc':'geohashed_end_loc'},inplace=True)\n",
    "    result = pd.merge(result, user_sloc_count, on=['userid', 'geohashed_end_loc'], how='left')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取用户从这个路径走过几次\n",
    "def get_user_sloc_eloc_count(train,result):\n",
    "    user_count = train.groupby(['userid','geohashed_start_loc','geohashed_end_loc'],as_index=False)['userid'].agg({'user_sloc_eloc_count':'count'})\n",
    "    result = pd.merge(result,user_count,on=['userid','geohashed_start_loc','geohashed_end_loc'],how='left')\n",
    "    return result\n",
    "\n",
    "# 获取用户从这个路径折返过几次\n",
    "def get_user_eloc_sloc_count(train,result):\n",
    "    user_eloc_sloc_count = train.groupby(['userid','geohashed_start_loc','geohashed_end_loc'],as_index=False)['userid'].agg({'user_eloc_sloc_count':'count'})\n",
    "    user_eloc_sloc_count.rename(columns = {'geohashed_start_loc':'geohashed_end_loc','geohashed_end_loc':'geohashed_start_loc'},inplace=True)\n",
    "    result = pd.merge(result,user_eloc_sloc_count,on=['userid','geohashed_start_loc','geohashed_end_loc'],how='left')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取目标地点的热度(目的地)\n",
    "def get_eloc_count(train,result):\n",
    "    eloc_count = train.groupby('geohashed_end_loc', as_index=False)['userid'].agg({'eloc_count': 'count'})\n",
    "    result = pd.merge(result, eloc_count, on='geohashed_end_loc', how='left')\n",
    "    return result\n",
    "\n",
    "# 获取目标地点的热度(出发地地)\n",
    "def get_eloc_as_sloc_count(train,result):\n",
    "    eloc_as_sloc_count = train.groupby('geohashed_start_loc', as_index=False)['userid'].agg({'eloc_as_sloc_count': 'count'})\n",
    "    eloc_as_sloc_count.rename(columns={'geohashed_start_loc':'geohashed_start_loc'})\n",
    "    result = pd.merge(result, eloc_as_sloc_count, on='geohashed_start_loc', how='left')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得某个时间段，达到某个地方，或者是从某个时间段出发的信息\n",
    "def hour_loc_count(train,result):\n",
    "    result['hour'] = pd.to_datetime(result['starttime']).map(lambda x: x.strftime('%H'))\n",
    "    train['hour'] = pd.to_datetime(train['starttime']).map(lambda x: x.strftime('%H'))\n",
    "            \n",
    "    hour_sloc_count = train.groupby([\"hour\",\"geohashed_start_loc\"],as_index=False)[\"userid\"].agg({'hour_sloc_count':'count'})\n",
    "    #某时，从某个地方出发的次数\n",
    "    hour_eloc_count = train.groupby([\"hour\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'hour_eloc_count':'count'})\n",
    "    #某时，到达某地的次数\n",
    "    hour_sloc_eloc_count = train.groupby([\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'hour_sloc_eloc_count':'count'})\n",
    "    #某时，某地出发，到达某地的次数\n",
    "    user_hour = train.groupby([\"userid\",\"hour\"],as_index=False)[\"userid\"].agg({'hour_user_count':'count'})\n",
    "    #用户从某个时间出发的次数\n",
    "    user_hour_eloc = train.groupby([\"userid\",\"hour\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'user_hour_eloc':'count'})\n",
    "    #用户某时到某地的次数\n",
    "    user_hour_sloc = train.groupby([\"userid\",\"hour\",\"geohashed_start_loc\"],as_index=False)[\"userid\"].agg({'user_hour_sloc':'count'})\n",
    "    #用户某时从某地出发的次数\n",
    "    user_hour_sloc_eloc = train.groupby([\"userid\",\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"],as_index=False)[\"userid\"].agg({'user_hour_sloc_eloc':'count'})\n",
    "    #用户某时从某地出发到达某地的次数\n",
    "    \n",
    "    result = pd.merge(result, hour_sloc_count, on=[\"hour\",\"geohashed_start_loc\"], how='left')\n",
    "    result = pd.merge(result, hour_eloc_count, on=[\"hour\",\"geohashed_end_loc\"], how='left')\n",
    "    result = pd.merge(result, hour_sloc_eloc_count, on=[\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"], how='left')\n",
    "    result = pd.merge(result, user_hour, on=[\"userid\",\"hour\"], how='left')\n",
    "    result = pd.merge(result, user_hour_eloc, on=[\"userid\",\"hour\",\"geohashed_end_loc\"], how='left')\n",
    "    result = pd.merge(result, user_hour_sloc, on=[\"userid\",\"hour\",\"geohashed_start_loc\"], how='left')\n",
    "    result = pd.merge(result, user_hour_sloc_eloc, on=[\"userid\",\"hour\",\"geohashed_start_loc\",\"geohashed_end_loc\"], how='left')\n",
    "    \n",
    "    result[\"hour\"] = result[\"hour\"].astype(int)\n",
    "    result.drop([\"starttime\"],axis=1,inplace=True)\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下部分，为样本构造部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得训练集的basic\n",
    "def get_train_basic():\n",
    "    bs_feat = ['orderid','geohashed_end_loc']\n",
    "    result_path = cache_path + 'get_train_basic_%s.hdf' %(\"train_bs\")\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        val = train[(train['starttime']>= split_time)]\n",
    "        train = train[(train['starttime'] < split_time)]\n",
    "                \n",
    "        user_end_loc = get_user_end_loc(train, val)# 根据用户历史目的地点添加样本\n",
    "        user_start_loc = get_user_start_loc(train, val)# 根据用户历史起始地点添加样本\n",
    "        loc_to_loc = get_loc_to_loc(train, val)# 筛选起始地点去向最多的3个地点\n",
    "        bike_end_loc = get_bike_end_loc(train, val)#车ID中，多次重复去同一个地方的样本\n",
    "        user_go_back = get_user_go_back(train, val)#用户存在往返的样本 \n",
    "        \n",
    "        result = pd.concat([user_end_loc[bs_feat],\n",
    "                            user_start_loc[bs_feat],\n",
    "                            loc_to_loc[bs_feat],\n",
    "                            bike_end_loc[bs_feat],\n",
    "                            user_go_back[bs_feat],]).drop_duplicates()\n",
    "        \n",
    "        val.drop([\"geohashed_end_loc\"],axis=1,inplace=True)\n",
    "        result = pd.merge(result,val,on=\"orderid\",how=\"left\")\n",
    "        result = result[result['geohashed_end_loc'] != result['geohashed_start_loc']]\n",
    "        result = result[(~result['geohashed_end_loc'].isnull()) & (~result['geohashed_start_loc'].isnull())]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "##获得训练集的basic\n",
    "def get_test_basic():\n",
    "    bs_feat = ['orderid','geohashed_end_loc']\n",
    "    result_path = cache_path + 'get_test_basic_%s.hdf' %(\"test_bs\")\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        train = train[(train['starttime'] < split_time)]\n",
    "        test = pd.read_csv(test_path)\n",
    "            \n",
    "        user_end_loc = get_user_end_loc(train, test) # 根据用户历史目的地点添加样本\n",
    "        user_start_loc = get_user_start_loc(train, test)# 根据用户历史起始地点添加样本 \n",
    "        loc_to_loc = get_loc_to_loc(train, test)  # 筛选起始地点去向最多的3个地点\n",
    "        bike_end_loc = get_bike_end_loc(train, test)#车ID中，多次重复去同一个地方的样本\n",
    "        user_go_back = get_user_go_back(train, test)#用户存在往返的样本 \n",
    "        # 汇总样本id\n",
    "        result = pd.concat([user_end_loc[bs_feat],\n",
    "                            user_start_loc[bs_feat],\n",
    "                            loc_to_loc[bs_feat],\n",
    "                            bike_end_loc[bs_feat],\n",
    "                            user_go_back[bs_feat],]).drop_duplicates()\n",
    "        \n",
    "        result = pd.merge(result,test,on=\"orderid\",how=\"left\")\n",
    "        result = result[result['geohashed_end_loc'] != result['geohashed_start_loc']]\n",
    "        result = result[(~result['geohashed_end_loc'].isnull()) & (~result['geohashed_start_loc'].isnull())]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 将用户骑行过目的的地点加入样本\n",
    "def get_user_end_loc(train,sample):\n",
    "    result_path = cache_path + 'user_end_loc_%d.hdf' %(train.shape[0]*sample.shape[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        user_eloc = train[['userid','geohashed_end_loc']].drop_duplicates()\n",
    "        result = pd.merge(sample[['orderid','userid']],user_eloc,on='userid',how='left')\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 将用户骑行过出发的地点加入样本\n",
    "def get_user_start_loc(train,sample):\n",
    "    result_path = cache_path + 'user_start_loc_%d.hdf' %(train.shape[0]*sample.shape[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        user_sloc = train[['userid', 'geohashed_start_loc']].drop_duplicates()\n",
    "        result = pd.merge(sample[['orderid', 'userid']], user_sloc, on='userid', how='left')\n",
    "        result.rename(columns={'geohashed_start_loc':'geohashed_end_loc'},inplace=True)\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 筛选起始地点去向最多的5个地点\n",
    "def get_loc_to_loc(train,sample):\n",
    "    result_path = cache_path + 'loc_to_loc_%d.hdf' %(train.shape[0]*sample.shape[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        train.geohashed_start_loc = train.geohashed_start_loc.apply(lambda x:x[:6])\n",
    "        sample.geohashed_start_loc = sample.geohashed_start_loc.apply(lambda x:x[:6])\n",
    "        #出发点，只取前6位\n",
    "        sloc_eloc_count = train.groupby(['geohashed_start_loc', 'geohashed_end_loc'],as_index=False)['geohashed_end_loc'].agg({'sloc_eloc_count':'count'})\n",
    "        sloc_eloc_count.sort_values('sloc_eloc_count',inplace=True)\n",
    "        sloc_eloc_count = sloc_eloc_count.groupby('geohashed_start_loc').tail(5)\n",
    "        \n",
    "        result = pd.merge(sample[['orderid', 'geohashed_start_loc']], sloc_eloc_count, on='geohashed_start_loc', how='left')\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "# 将车ID多次重复去一个地方的样本取出\n",
    "def get_bike_end_loc(train,sample):\n",
    "    result_path = cache_path + 'bike_end_loc_%d.hdf' %(train.shape[0]*sample.shape[0])\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        bike_eloc = train.groupby([\"bikeid\",\"geohashed_end_loc\"],as_index=False)[\"geohashed_end_loc\"].agg({\"bike_count\":\"count\"})\n",
    "        bike_eloc = bike_eloc[bike_eloc.bike_count>2][[\"bikeid\",\"geohashed_end_loc\"]]\n",
    "        result = pd.merge(sample[['orderid','bikeid']],bike_eloc,on='bikeid',how='left')\n",
    "        result = result[['orderid', 'geohashed_end_loc']]\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result\n",
    "\n",
    "#将用户往返过的地方，放入样本\n",
    "def get_user_go_back(train,sample):\n",
    "    result_path = cache_path + 'user_go_back_%d.hdf' %(train.shape[0]*sample.shape[0])\n",
    "    \n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        user_go_back = pd.merge(train[[\"orderid\",\"userid\",\"geohashed_start_loc\",\"geohashed_end_loc\"]],\n",
    "                                train[[\"orderid\",\"userid\",\"geohashed_start_loc\",\"geohashed_end_loc\"]],\n",
    "                                left_on=[\"userid\",\"geohashed_start_loc\",\"geohashed_end_loc\"],\n",
    "                                right_on=[\"userid\",\"geohashed_end_loc\",\"geohashed_start_loc\"],\n",
    "                                how =\"inner\")\n",
    "        \n",
    "        user_go_back = user_go_back[[\"userid\",\"geohashed_start_loc_x\",\"geohashed_end_loc_x\"]].drop_duplicates()\n",
    "        \n",
    "        result1 = pd.merge(sample[['orderid','userid',\"geohashed_start_loc\"]],\n",
    "                          user_go_back,\n",
    "                          left_on = [\"userid\",\"geohashed_start_loc\"],\n",
    "                          right_on= [\"userid\",\"geohashed_start_loc_x\"],\n",
    "                          how=\"inner\")\n",
    "        result1.rename(columns={'geohashed_end_loc_x':'geohashed_end_loc'},inplace=True)\n",
    "        \n",
    "        result2 = pd.merge(sample[['orderid','userid',\"geohashed_start_loc\"]],\n",
    "                          user_go_back,\n",
    "                          left_on = [\"userid\",\"geohashed_start_loc\"],\n",
    "                          right_on= [\"userid\",\"geohashed_end_loc_x\"],\n",
    "                          how=\"inner\")\n",
    "        result2.rename(columns={'geohashed_start_loc_x':'geohashed_end_loc'},inplace=True)\n",
    "        \n",
    "        result = pd.concat([result1,result2],axis=0)\n",
    "        \n",
    "        result = result[['orderid', 'geohashed_end_loc']].drop_duplicates()\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tr_data():\n",
    "    \n",
    "    result_path = cache_path + 'make_tr_data.hdf'\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "   \n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        train = train[(train['starttime'] < split_time)]\n",
    "        \n",
    "        result = get_train_basic()\n",
    "        result = hour_loc_count(train,result)                                   # 处理时间相关特征\n",
    "        result = get_user_count(train,result)                                   # 获取用户历史行为次数\n",
    "        result = get_user_eloc_count(train, result)                             # 获取用户去过这个地点几次\n",
    "        result = get_user_sloc_count(train, result)                             # 获取用户从目的地点出发过几次\n",
    "        result = get_user_sloc_eloc_count(train, result)                        # 获取用户从这个路径走过几次\n",
    "        result = get_user_eloc_sloc_count(train, result)                        # 获取用户从这个路径折返过几次\n",
    "        result = get_eloc_count(train, result)                                  # 获取目的地点的热度(目的地)\n",
    "        result = get_eloc_as_sloc_count(train, result)                          # 获取起始点和最终地点的欧式距离\n",
    "        result = get_distance(result)                                           \n",
    "        result = result.fillna(0)\n",
    "        result = get_label(result)\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ts_data():\n",
    "    result_path = cache_path + 'make_ts_data.hdf'\n",
    "    if os.path.exists(result_path) & flag:\n",
    "        result = pd.read_hdf(result_path, 'w')\n",
    "    else:\n",
    "        train = pd.read_csv(train_path)\n",
    "        train = train[(train['starttime'] < split_time)]\n",
    "        \n",
    "        result = get_test_basic()\n",
    "        result = hour_loc_count(train,result)                                   # 处理时间相关特征\n",
    "        result = get_user_count(train,result)                                   # 获取用户历史行为次数\n",
    "        result = get_user_eloc_count(train, result)                             # 获取用户去过这个地点几次\n",
    "        result = get_user_sloc_count(train, result)                             # 获取用户从目的地点出发过几次\n",
    "        result = get_user_sloc_eloc_count(train, result)                        # 获取用户从这个路径走过几次\n",
    "        result = get_user_eloc_sloc_count(train, result)                        # 获取用户从这个路径折返过几次\n",
    "        result = get_eloc_count(train, result)                                  # 获取目的地点的热度(目的地)\n",
    "        result = get_eloc_as_sloc_count(train, result)\n",
    "        result = get_distance(result)                                           # 获取起始点和最终地点的欧式距离\n",
    "        result = result.fillna(0)\n",
    "        result = get_label(result)\n",
    "        result.to_hdf(result_path, 'w', complib='blosc', complevel=5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分组排序\n",
    "def rank(data, feat1, feat2):\n",
    "    data.sort_values([feat1,feat2],inplace=True,ascending=False)\n",
    "    data['rank'] = range(data.shape[0])\n",
    "    min_rank = data.groupby(feat1,as_index=False)['rank'].agg({'min_rank':'min'})\n",
    "    data = pd.merge(data,min_rank,on=feat1,how='left')\n",
    "    data['rank'] = data['rank'] - data['min_rank']\n",
    "    del data['min_rank']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对结果进行整理\n",
    "def reshape(pred):\n",
    "    result = pred.copy()\n",
    "    result = rank(result,'orderid','pred')\n",
    "    result = result[result['rank']<3][['orderid','geohashed_end_loc','rank']]\n",
    "    result = result.set_index(['orderid','rank']).unstack()\n",
    "    result.reset_index(inplace=True)\n",
    "    result['orderid'] = result['orderid'].astype('int')\n",
    "    result.columns = ['orderid', 0, 1, 2]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#训练样本\n",
    "from xgboost import XGBClassifier    \n",
    "train_feat = make_tr_data()\n",
    "test_feat = make_ts_data()\n",
    "\n",
    "feat = [\"orderid\",\"geohashed_end_loc\",\"geohashed_start_loc\",\"userid\",\"bikeid\"];label = [\"label\"]\n",
    "\n",
    "predictors = [_ for _ in train_feat.columns if _ not in feat+label]\n",
    "\n",
    "clf = XGBClassifier(objective ='binary:logistic',\n",
    "                            learning_rate = 0.1,\n",
    "                            silent=0,\n",
    "                            n_estimators= 500,\n",
    "                            max_depth = 10,\n",
    "                            min_child_weight = 2,\n",
    "                            gamma = 30,\n",
    "                            reg_alpha = 10,\n",
    "                            reg_lambda = 50,                        \n",
    "                            subsample = 0.886, \n",
    "                            colsample_bytree = 0.886,\n",
    "                            scale_pos_weight = 10,\n",
    "                            nthread =4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_feat[predictors].values, \n",
    "                                                    train_feat[label].values, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=201709)\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train,y_train),(X_test, y_test)], \n",
    "        eval_metric= [\"auc\",\"map\"],\n",
    "        early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feat['pred'] = clf.predict_proba(test_feat[predictors])[:,1]\n",
    "result = reshape(test_feat)\n",
    "test = pd.read_csv(test_path)\n",
    "result = pd.merge(test[['orderid']],result,on='orderid',how='left')\n",
    "result.fillna('0',inplace=True)\n",
    "result.to_csv('../sub/result.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.figure(figsize=(20,4))\n",
    "feat_imp = pd.Series(clf.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
